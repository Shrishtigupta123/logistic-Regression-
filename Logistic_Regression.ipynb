{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "                    **Theoretical**\n",
        "\n",
        "\n",
        "1 What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "-### **Logistic Regression vs. Linear Regression**\n",
        "\n",
        "Both **Logistic Regression** and **Linear Regression** are supervised learning algorithms used in statistical modeling and machine learning. However, they are used for different types of tasks and have distinct characteristics.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Logistic Regression**\n",
        "- **Purpose**: Used for **classification problems** (especially binary classification).\n",
        "- **Output**: Produces probabilities (values between 0 and 1) which are mapped to classes.\n",
        "- **Mathematical Model**:\n",
        "  - Instead of modeling a straight line, it applies the **sigmoid function (logistic function)** to restrict output values between 0 and 1:\n",
        "    \\[\n",
        "    P(Y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n",
        "    \\]\n",
        "  - The decision boundary is determined based on a threshold (e.g., 0.5 for binary classification).\n",
        "- **Types**:\n",
        "  - **Binary Logistic Regression** (two classes)\n",
        "  - **Multinomial Logistic Regression** (multiple classes)\n",
        "  - **Ordinal Logistic Regression** (ordered classes)\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Linear Regression**\n",
        "- **Purpose**: Used for **regression problems** (predicting continuous values).\n",
        "- **Output**: Produces real-valued outputs (e.g., predicting house prices, stock prices).\n",
        "- **Mathematical Model**:\n",
        "  - Models a straight-line relationship between input \\(X\\) and output \\(Y\\):\n",
        "    \\[\n",
        "    Y = \\beta_0 + \\beta_1 X + \\epsilon\n",
        "    \\]\n",
        "  - The coefficients (\\(\\beta_0, \\beta_1\\)) are estimated using the **least squares method**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Key Differences**\n",
        "| Feature             | Logistic Regression           | Linear Regression           |\n",
        "|---------------------|-----------------------------|-----------------------------|\n",
        "| **Type of Problem** | Classification (e.g., Yes/No, Spam/Not Spam) | Regression (e.g., Price Prediction) |\n",
        "| **Output Values** | Probabilities (0 to 1) | Continuous values (any real number) |\n",
        "| **Function Used** | Sigmoid function | Linear function |\n",
        "| **Decision Boundary** | Threshold-based (e.g., \\( P > 0.5 \\Rightarrow 1, P \\leq 0.5 \\Rightarrow 0 \\)) | Directly predicts values |\n",
        "| **Error Measurement** | Log-loss (cross-entropy loss) | Mean Squared Error (MSE) |\n",
        "\n",
        "---\n",
        "\n",
        "### **4. When to Use Which?**\n",
        "- Use **Logistic Regression** when you need to classify data into categories (e.g., fraud detection, spam classification).\n",
        "- Use **Linear Regression** when you need to predict continuous outcomes (e.g., predicting sales revenue, house prices).\n",
        "\n",
        "\n",
        "\n",
        "2 What is the mathematical equation of Logistic Regression?\n",
        "-The **mathematical equation of Logistic Regression** is based on the **sigmoid function (logistic function)**, which maps any real-valued number to a value between 0 and 1. The equation is:\n",
        "\n",
        "### **1. Probability Estimation**\n",
        "For a given input \\( X \\), the probability that the output \\( Y \\) belongs to class 1 (instead of class 0) is given by:\n",
        "\n",
        "\\[\n",
        "P(Y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n)}}\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\( P(Y = 1 | X) \\) is the probability that \\( Y \\) belongs to class 1.\n",
        "- \\( \\beta_0 \\) is the intercept (bias).\n",
        "- \\( \\beta_1, \\beta_2, ..., \\beta_n \\) are the regression coefficients (weights).\n",
        "- \\( X_1, X_2, ..., X_n \\) are the input features.\n",
        "- \\( e \\) is Euler‚Äôs number (\\(\\approx 2.718\\)).\n",
        "\n",
        "This is the **sigmoid function**:\n",
        "\n",
        "\\[\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\]\n",
        "\n",
        "where \\( z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n \\).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Decision Boundary**\n",
        "- If \\( P(Y = 1 | X) \\geq 0.5 \\), we classify \\( Y = 1 \\).\n",
        "- If \\( P(Y = 1 | X) < 0.5 \\), we classify \\( Y = 0 \\).\n",
        "\n",
        "This decision threshold can be adjusted based on the problem.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Log-Odds Form (Logit Function)**\n",
        "Taking the **log-odds (logit function)** of the probability:\n",
        "\n",
        "\\[\n",
        "\\log \\left( \\frac{P(Y=1 | X)}{1 - P(Y=1 | X)} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n\n",
        "\\]\n",
        "\n",
        "This shows that Logistic Regression models a **linear relationship between the log-odds of the outcome and the input features**.\n",
        "\n",
        "\n",
        "\n",
        "3 Why do we use the Sigmoid function in Logistic Regression?\n",
        "-### **Why Do We Use the Sigmoid Function in Logistic Regression?**\n",
        "\n",
        "The **Sigmoid function** (also called the **logistic function**) is used in Logistic Regression because it helps map any real-valued number to a probability between **0 and 1**, making it ideal for **classification tasks**.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Definition of Sigmoid Function**\n",
        "The Sigmoid function is mathematically defined as:\n",
        "\n",
        "\\[\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\( z \\) is the linear combination of input features and weights:\n",
        "  \n",
        "  \\[\n",
        "  z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n\n",
        "  \\]\n",
        "\n",
        "- \\( e \\) is Euler‚Äôs number (\\(\\approx 2.718\\)).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Key Reasons for Using Sigmoid in Logistic Regression**\n",
        "#### **(a) Converts Any Input into a Probability (0 to 1)**\n",
        "- The output of the Sigmoid function is always in the range \\( (0,1) \\), making it suitable for **probability estimation**.\n",
        "- A value closer to 1 means **higher confidence** in class **1**, while a value closer to 0 means **higher confidence** in class **0**.\n",
        "\n",
        "#### **(b) Introduces Non-linearity**\n",
        "- Logistic Regression models a linear relationship between input features and the **log-odds** of the outcome.\n",
        "- Applying the **sigmoid function** ensures that the final output is **non-linear**, which helps in classification.\n",
        "\n",
        "#### **(c) Helps in Decision Making**\n",
        "- We can set a decision threshold (e.g., **0.5**).\n",
        "  - If \\( \\sigma(z) \\geq 0.5 \\), classify as **1**.\n",
        "  - If \\( \\sigma(z) < 0.5 \\), classify as **0**.\n",
        "- This makes it easy to separate two classes.\n",
        "\n",
        "#### **(d) Differentiable and Enables Gradient Descent Optimization**\n",
        "- The Sigmoid function is **smooth and differentiable**, which is essential for **gradient descent** to update the model parameters efficiently.\n",
        "- Its derivative is:\n",
        "\n",
        "  \\[\n",
        "  \\sigma'(z) = \\sigma(z) (1 - \\sigma(z))\n",
        "  \\]\n",
        "\n",
        "  This property makes it computationally efficient for training the model.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Visualization of the Sigmoid Function**\n",
        "If we plot \\( \\sigma(z) \\), we get an S-shaped curve:\n",
        "\n",
        "\\[\n",
        "\\begin{array}{c|c}\n",
        "z & \\sigma(z) \\\\\n",
        "\\hline\n",
        "- \\infty & 0 \\\\\n",
        "0 & 0.5 \\\\\n",
        "+ \\infty & 1 \\\\\n",
        "\\end{array}\n",
        "\\]\n",
        "\n",
        "- For large **negative** values of \\( z \\), \\( \\sigma(z) \\approx 0 \\) (predicts class **0**).\n",
        "- For large **positive** values of \\( z \\), \\( \\sigma(z) \\approx 1 \\) (predicts class **1**).\n",
        "- At \\( z = 0 \\), \\( \\sigma(z) = 0.5 \\), meaning equal probability for both classes.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "The Sigmoid function is used in Logistic Regression because:\n",
        "1. It **squashes outputs** into a probability range (0 to 1).\n",
        "2. It **introduces non-linearity**, making it suitable for classification.\n",
        "3. It provides a **natural decision boundary** for classification.\n",
        "4. It allows for **gradient-based optimization**.\n",
        "\n",
        "\n",
        "\n",
        "4 What is the cost function of Logistic Regression?\n",
        "-### **Cost Function of Logistic Regression**\n",
        "The cost function in Logistic Regression measures how well the model's predictions match the actual labels. Instead of using the **Mean Squared Error (MSE)** (as in Linear Regression), we use the **Log Loss (Cross-Entropy Loss)** because Logistic Regression deals with probabilities and classification.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Why Not Use Mean Squared Error (MSE)?**\n",
        "- If we use **MSE**, the optimization function becomes **non-convex** for Logistic Regression, making it difficult for gradient descent to find the global minimum.\n",
        "- The Sigmoid function is **non-linear**, so MSE doesn't work well for classification.\n",
        "\n",
        "Instead, we use a loss function derived from **Maximum Likelihood Estimation (MLE)**, which leads to the **Log Loss (Cross-Entropy Loss)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Log Loss (Cross-Entropy Loss)**\n",
        "For a binary classification problem, where:\n",
        "- \\( y = 1 \\) represents the positive class.\n",
        "- \\( y = 0 \\) represents the negative class.\n",
        "- \\( \\hat{y} \\) (or \\( P(Y=1 | X) \\)) is the predicted probability from the sigmoid function.\n",
        "\n",
        "The **cost function for a single training example** is:\n",
        "\n",
        "\\[\n",
        "J(\\beta) = - \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
        "\\]\n",
        "\n",
        "- If **\\( y = 1 \\)** (positive class):  \n",
        "  \\[\n",
        "  J(\\beta) = - \\log(\\hat{y})\n",
        "  \\]\n",
        "  - If \\( \\hat{y} \\) is close to **1** ‚Üí **loss is small**.\n",
        "  - If \\( \\hat{y} \\) is close to **0** ‚Üí **loss is large**.\n",
        "\n",
        "- If **\\( y = 0 \\)** (negative class):  \n",
        "  \\[\n",
        "  J(\\beta) = - \\log(1 - \\hat{y})\n",
        "  \\]\n",
        "  - If \\( \\hat{y} \\) is close to **0** ‚Üí **loss is small**.\n",
        "  - If \\( \\hat{y} \\) is close to **1** ‚Üí **loss is large**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Cost Function for the Entire Dataset**\n",
        "For **\\( m \\) training examples**, the total cost function is:\n",
        "\n",
        "\\[\n",
        "J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\( m \\) = number of training samples.\n",
        "- \\( y^{(i)} \\) = actual label of the \\( i^{th} \\) example.\n",
        "- \\( \\hat{y}^{(i)} \\) = predicted probability for the \\( i^{th} \\) example.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Properties of Log Loss**\n",
        "- **Convex Function**: Ensures a global minimum, making it suitable for optimization using **Gradient Descent**.\n",
        "- **Punishes Incorrect Predictions Heavily**: If the model is very confident but wrong, the loss is large.\n",
        "- **Encourages Probabilistic Confidence**: The model learns to output high probabilities for correct predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Optimization Using Gradient Descent**\n",
        "To minimize the cost function, we use **Gradient Descent**, updating the weights \\( \\beta \\) using:\n",
        "\n",
        "\\[\n",
        "\\beta_j := \\beta_j - \\alpha \\frac{\\partial J}{\\partial \\beta_j}\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\( \\alpha \\) = learning rate.\n",
        "- \\( \\frac{\\partial J}{\\partial \\beta_j} \\) = gradient of the cost function.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "- The **Log Loss (Cross-Entropy Loss)** is the best cost function for Logistic Regression.\n",
        "- It ensures that probabilities are well-calibrated and encourages the model to be confident in correct predictions.\n",
        "- We use **Gradient Descent** to minimize it efficiently.\n",
        "\n",
        "\n",
        "\n",
        "5  What is Regularization in Logistic Regression? Why is it needed?\n",
        "-### **Regularization in Logistic Regression**\n",
        "Regularization is a technique used in Logistic Regression to **prevent overfitting** by adding a penalty term to the cost function. This helps control the complexity of the model and improves its generalization to unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Why is Regularization Needed?**\n",
        "Without regularization, Logistic Regression can **overfit** the training data, especially when:\n",
        "- There are **too many features** (high-dimensional data).\n",
        "- The dataset is **small** relative to the number of features.\n",
        "- The model becomes too complex by assigning **large weights** to certain features.\n",
        "\n",
        "Regularization **shrinks the weights**, preventing them from becoming too large and making the model more robust.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Types of Regularization**\n",
        "Regularization is applied by adding a **penalty term** to the **cost function** of Logistic Regression.\n",
        "\n",
        "#### **(a) L1 Regularization (Lasso Regression)**\n",
        "- Uses the **absolute values** of the coefficients.\n",
        "- The modified cost function:\n",
        "\n",
        "  \\[\n",
        "  J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] + \\lambda \\sum_{j=1}^{n} |\\beta_j|\n",
        "  \\]\n",
        "\n",
        "- **Effect**: Drives some coefficients to **exactly zero**, effectively selecting important features (**feature selection**).\n",
        "- Useful when we expect **only a few features** to be important.\n",
        "\n",
        "---\n",
        "\n",
        "#### **(b) L2 Regularization (Ridge Regression)**\n",
        "- Uses the **squared values** of the coefficients.\n",
        "- The modified cost function:\n",
        "\n",
        "  \\[\n",
        "  J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] + \\lambda \\sum_{j=1}^{n} \\beta_j^2\n",
        "  \\]\n",
        "\n",
        "- **Effect**: Shrinks all coefficients but **does not set them to zero**.\n",
        "- Helps in handling **multicollinearity** (high correlation between features).\n",
        "\n",
        "---\n",
        "\n",
        "#### **(c) Elastic Net (Combination of L1 and L2)**\n",
        "- A mix of **L1 and L2 regularization**:\n",
        "\n",
        "  \\[\n",
        "  J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] + \\lambda_1 \\sum_{j=1}^{n} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{n} \\beta_j^2\n",
        "  \\]\n",
        "\n",
        "- **Effect**: Selects important features (like L1) while still keeping some small weights (like L2).\n",
        "- Useful when there are **many correlated features**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. How to Choose the Regularization Parameter (\\( \\lambda \\))?**\n",
        "- \\( \\lambda \\) controls the amount of regularization:\n",
        "  - **High \\( \\lambda \\)** ‚Üí More penalty ‚Üí **Simpler model (underfitting)**.\n",
        "  - **Low \\( \\lambda \\)** ‚Üí Less penalty ‚Üí **More complex model (overfitting)**.\n",
        "- The optimal \\( \\lambda \\) is chosen using **cross-validation**.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Conclusion**\n",
        "- Regularization is crucial in **high-dimensional datasets** to prevent overfitting.\n",
        "- **L1 Regularization (Lasso)** ‚Üí Feature selection (some coefficients become zero).\n",
        "- **L2 Regularization (Ridge)** ‚Üí Shrinks coefficients but keeps them all.\n",
        "- **Elastic Net** ‚Üí Combines both L1 and L2.\n",
        "\n",
        "\n",
        "6  Explain the difference between Lasso, Ridge, and Elastic Net regression\n",
        "-### **Difference Between Lasso, Ridge, and Elastic Net Regression**\n",
        "Lasso, Ridge, and Elastic Net are types of **regularization techniques** used in regression models (including **Logistic Regression** and **Linear Regression**) to prevent **overfitting** by adding a penalty to the model‚Äôs coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Ridge Regression (L2 Regularization)**\n",
        "- **Penalty Term: Sum of squared coefficients**\n",
        "  \n",
        "  \\[\n",
        "  J(\\beta) = \\text{Loss} + \\lambda \\sum_{j=1}^{n} \\beta_j^2\n",
        "  \\]\n",
        "  \n",
        "- **Effect on Coefficients**:\n",
        "  - Shrinks all coefficients towards zero **but does not make them exactly zero**.\n",
        "  - Retains all features but reduces their impact.\n",
        "\n",
        "- **Best For**:\n",
        "  - **Multicollinear Data** (when features are highly correlated).\n",
        "  - Situations where **all features contribute to prediction**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Lasso Regression (L1 Regularization)**\n",
        "- **Penalty Term: Sum of absolute values of coefficients**\n",
        "  \n",
        "  \\[\n",
        "  J(\\beta) = \\text{Loss} + \\lambda \\sum_{j=1}^{n} |\\beta_j|\n",
        "  \\]\n",
        "\n",
        "- **Effect on Coefficients**:\n",
        "  - Some coefficients are **shrunk to exactly zero**, effectively removing them.\n",
        "  - Performs **feature selection** by eliminating less important features.\n",
        "\n",
        "- **Best For**:\n",
        "  - When we suspect **only a few features are important**.\n",
        "  - High-dimensional datasets with **many irrelevant features**.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Elastic Net Regression (Combination of L1 & L2)**\n",
        "- **Penalty Term: Combination of L1 and L2**\n",
        "  \n",
        "  \\[\n",
        "  J(\\beta) = \\text{Loss} + \\lambda_1 \\sum_{j=1}^{n} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{n} \\beta_j^2\n",
        "  \\]\n",
        "\n",
        "- **Effect on Coefficients**:\n",
        "  - Selects important features like **Lasso (L1)**.\n",
        "  - Shrinks coefficients like **Ridge (L2)** but avoids some of Ridge‚Äôs issues with correlated features.\n",
        "\n",
        "- **Best For**:\n",
        "  - When there are **many correlated features**.\n",
        "  - Situations where we need **feature selection but don‚Äôt want too many zeros**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Comparison Table**\n",
        "\n",
        "| Feature           | Ridge (L2) | Lasso (L1) | Elastic Net (L1 + L2) |\n",
        "|------------------|-----------|------------|------------------|\n",
        "| **Regularization Type** | L2 (squared sum of coefficients) | L1 (absolute sum of coefficients) | Combination of L1 and L2 |\n",
        "| **Effect on Coefficients** | Shrinks coefficients but keeps all | Shrinks some coefficients to **zero** (feature selection) | Shrinks some coefficients to zero while keeping others small |\n",
        "| **Feature Selection?** | **No** (keeps all features) | **Yes** (eliminates some features) | **Yes** (controlled feature selection) |\n",
        "| **Best When** | Features are correlated & all are important | Some features are irrelevant | Features are correlated & some need to be eliminated |\n",
        "| **Computational Complexity** | Low | High (solving L1 norm is harder) | Moderate |\n",
        "\n",
        "---\n",
        "\n",
        "## **Which One to Use?**\n",
        "- **Use Ridge (L2)** if all features are relevant but need to be controlled (e.g., preventing multicollinearity).\n",
        "- **Use Lasso (L1)** if you want to automatically select important features.\n",
        "- **Use Elastic Net** if there are **many correlated features** and you want a balance between feature selection & shrinkage.\n",
        "\n",
        "\n",
        "7 When should we use Elastic Net instead of Lasso or Ridge?\n",
        "-### **When to Use Elastic Net Instead of Lasso or Ridge?**  \n",
        "\n",
        "Elastic Net is a combination of **Lasso (L1)** and **Ridge (L2)** regularization. It is particularly useful in scenarios where neither Lasso nor Ridge alone performs optimally.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. When Features Are Highly Correlated (Multicollinearity)**\n",
        "- **Lasso (L1) struggles** when features are highly correlated because it randomly selects one feature and eliminates the others.\n",
        "- **Ridge (L2) keeps all features**, reducing the impact of multicollinearity but not performing feature selection.\n",
        "- **Elastic Net solves this by grouping correlated features together** rather than arbitrarily selecting one.\n",
        "\n",
        "üí° *Example:* If multiple features in a dataset represent similar information (e.g., different but related economic indicators), Elastic Net ensures they are either all included with reduced impact or all excluded.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. When You Need Feature Selection but Also Want to Keep Some Features**\n",
        "- **Lasso may remove too many features**, leading to underfitting.\n",
        "- **Ridge keeps all features**, making interpretation harder.\n",
        "- **Elastic Net provides a balance**, selecting important features while allowing small coefficients for others.\n",
        "\n",
        "üí° *Example:* In gene selection problems, some genes may be completely irrelevant (need to be removed), but others might have **small but meaningful effects**. Elastic Net prevents Lasso from discarding too many relevant genes.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. When Lasso Struggles with High-Dimensional Data (More Features Than Samples)**\n",
        "- **Lasso selects only a few features**, and if there are more features than samples (\\( p > n \\)), it may be unstable.\n",
        "- **Elastic Net stabilizes feature selection by combining L1 and L2 penalties**.\n",
        "\n",
        "üí° *Example:* If you have thousands of predictors (e.g., text classification with word features), Elastic Net is **more stable** than Lasso.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. When Ridge Doesn't Provide Enough Sparsity**\n",
        "- **Ridge reduces coefficient sizes but does not force any to zero**.\n",
        "- **Elastic Net encourages sparsity**, meaning **some features are eliminated while others are retained**.\n",
        "\n",
        "üí° *Example:* If you have **irrelevant features** in your dataset, Ridge will still give them small weights, but Elastic Net will remove them.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. When You Want a Tunable Balance Between L1 and L2**\n",
        "- Elastic Net has **two hyperparameters**:\n",
        "  - \\( \\lambda_1 \\) (L1 penalty) controls feature selection.\n",
        "  - \\( \\lambda_2 \\) (L2 penalty) controls weight shrinkage.\n",
        "- This allows fine-tuning **depending on how sparse you want your model to be**.\n",
        "\n",
        "üí° *Example:* If you want **more feature selection**, increase \\( \\lambda_1 \\). If you want **less aggressive shrinkage**, increase \\( \\lambda_2 \\).\n",
        "\n",
        "---\n",
        "\n",
        "## **Summary Table**\n",
        "\n",
        "| Scenario | Ridge (L2) | Lasso (L1) | Elastic Net (L1 + L2) |\n",
        "|----------|------------|------------|----------------|\n",
        "| **Multicollinearity (Correlated Features)** | Keeps all features but shrinks them | Eliminates some correlated features arbitrarily | **Keeps correlated features together** |\n",
        "| **Feature Selection Needed** | No | Yes (many coefficients become 0) | **Yes (but more stable than Lasso)** |\n",
        "| **High-Dimensional Data (\\( p > n \\))** | No | May select too few features | **More stable than Lasso** |\n",
        "| **Sparse Model (Few Features Used)** | No | Yes | **Yes (but with better control)** |\n",
        "| **Completely Irrelevant Features** | Small weights assigned | Removed | **Removed efficiently** |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Elastic Net?**\n",
        "‚úÖ **Use Elastic Net instead of Lasso or Ridge when**:\n",
        "- You have **many correlated features**.\n",
        "- Lasso is **too aggressive** in eliminating features.\n",
        "- Ridge keeps **too many unnecessary features**.\n",
        "- Your dataset is **high-dimensional** with **more features than samples**.\n",
        "- You want **better stability** in feature selection.\n",
        "\n",
        "\n",
        "8  What is the impact of the regularization parameter (Œª) in Logistic Regression?\n",
        "-### **Impact of the Regularization Parameter (Œª) in Logistic Regression**  \n",
        "\n",
        "In **Logistic Regression**, the regularization parameter **\\( \\lambda \\)** controls the **trade-off between model complexity and performance**. It determines how much penalty is applied to the model's coefficients (weights), preventing overfitting or underfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. What Does \\( \\lambda \\) Do?**\n",
        "- **\\( \\lambda \\) is a hyperparameter** that **controls the strength of regularization**.\n",
        "- It affects how much the model penalizes large weights.\n",
        "\n",
        "Mathematically, the **regularized cost function** in Logistic Regression is:\n",
        "\n",
        "\\[\n",
        "J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] + \\lambda \\sum_{j=1}^{n} R(\\beta_j)\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- **\\( R(\\beta_j) \\)** is the regularization term:\n",
        "  - **L1 (Lasso)**: \\( \\sum_{j=1}^{n} |\\beta_j| \\) (absolute values of weights).\n",
        "  - **L2 (Ridge)**: \\( \\sum_{j=1}^{n} \\beta_j^2 \\) (squared weights).\n",
        "- **\\( \\lambda \\) controls the importance of regularization**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Effects of \\( \\lambda \\)**\n",
        "| **\\( \\lambda \\) Value** | **Effect on Model** | **Overfitting vs Underfitting** | **Impact on Weights** |\n",
        "|----------------|------------------|----------------------|----------------|\n",
        "| **\\( \\lambda = 0 \\)** | No regularization | High risk of **overfitting** | Large coefficients |\n",
        "| **Small \\( \\lambda \\)** | Slight penalty on large weights | Some risk of overfitting | Weights slightly reduced |\n",
        "| **Optimal \\( \\lambda \\)** | Best balance between bias & variance | Best generalization | Moderate-sized weights |\n",
        "| **Large \\( \\lambda \\)** | Strong regularization | High risk of **underfitting** | Shrinks weights close to **zero** |\n",
        "| **Very large \\( \\lambda \\)** | Extreme penalty, model too simple | Severe **underfitting** | Coefficients **near zero** |\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Choosing the Right \\( \\lambda \\)**\n",
        "- **Too small \\( \\lambda \\) (or 0)** ‚Üí The model memorizes training data (**overfits**).\n",
        "- **Too large \\( \\lambda \\)** ‚Üí The model is too simple and ignores useful features (**underfits**).\n",
        "- **Best \\( \\lambda \\)** ‚Üí Found using **cross-validation** (e.g., Grid Search or Random Search).\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Visualizing the Impact of \\( \\lambda \\)**\n",
        "- **\\( \\lambda = 0 \\)** ‚Üí Model fits perfectly to training data but generalizes poorly.\n",
        "- **\\( \\lambda \\) is too high** ‚Üí Model loses flexibility and predicts poorly.\n",
        "\n",
        "üìâ A graph of \\( \\lambda \\) vs. model accuracy typically looks **like an inverted U**:  \n",
        "- Small \\( \\lambda \\) ‚Üí **Overfitting**.\n",
        "- Large \\( \\lambda \\) ‚Üí **Underfitting**.\n",
        "- **Optimal \\( \\lambda \\) lies in between**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Summary**\n",
        "- **\\( \\lambda \\) controls regularization strength** in Logistic Regression.\n",
        "- **Small \\( \\lambda \\)** ‚Üí Risk of **overfitting** (high variance).\n",
        "- **Large \\( \\lambda \\)** ‚Üí Risk of **underfitting** (high bias).\n",
        "- **Tuned using cross-validation** to find the best balance.\n",
        "\n",
        "\n",
        "9 What are the key assumptions of Logistic Regression?\n",
        "-### **Key Assumptions of Logistic Regression**  \n",
        "\n",
        "Logistic Regression is a widely used classification algorithm, but for it to work effectively, certain assumptions need to hold. Unlike **Linear Regression**, it does **not** require a strict assumption of linearity between the independent and dependent variables, but it does have other important considerations.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Dependent Variable is Binary (for Binary Logistic Regression)**\n",
        "- Logistic Regression assumes that the **target variable (Y) is binary** (0 or 1).\n",
        "- If dealing with **multiclass classification**, **Multinomial Logistic Regression** or **One-vs-All (OvR) strategy** is needed.\n",
        "\n",
        "üí° *Example:* Predicting whether an email is **spam (1) or not spam (0)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Independence of Observations**\n",
        "- The observations should be **independent** of each other.\n",
        "- Logistic Regression does **not work well with correlated observations**, such as time-series data unless adjustments are made (e.g., using autoregressive models).\n",
        "\n",
        "üí° *Example:* If predicting whether a patient has a disease, **each patient‚Äôs data should be independent** (not repeated measurements from the same individual).\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Linearity of Log-Odds (Logit Transformation)**\n",
        "- While Logistic Regression **does not assume a linear relationship between X and Y**, it assumes that the **log-odds (logit function) of the dependent variable is linearly related to the independent variables**.\n",
        "\n",
        "Mathematically:\n",
        "\\[\n",
        "\\log \\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n\n",
        "\\]\n",
        "\n",
        "üí° *Solution:* If this assumption is violated, we can:\n",
        "  - Use **polynomial terms or interactions**.\n",
        "  - Apply **nonlinear transformations** (e.g., log, square root).\n",
        "  - Use **more flexible models** like Decision Trees or Neural Networks.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. No Perfect Multicollinearity**\n",
        "- Features should **not be highly correlated** with each other.\n",
        "- **Multicollinearity** makes it difficult to estimate individual feature coefficients.\n",
        "\n",
        "üí° *Detection & Solution:*  \n",
        "- **Check correlation matrix** or use **Variance Inflation Factor (VIF)**.\n",
        "- **Drop one of the correlated features** or use **Principal Component Analysis (PCA)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Large Sample Size & Sufficient Data**\n",
        "- Logistic Regression performs better with **a large number of observations**.\n",
        "- Small sample sizes can lead to **unstable estimates**.\n",
        "\n",
        "üí° *Rule of Thumb:* Each independent variable should have **at least 10 observations per class**.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. No Strong Outliers**\n",
        "- Logistic Regression is **sensitive to outliers**, which can significantly affect the decision boundary.\n",
        "\n",
        "üí° *Solution:*  \n",
        "- **Detect outliers** using boxplots or Z-scores.  \n",
        "- **Transform or remove outliers** if necessary.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Independence of Errors (No Autocorrelation)**\n",
        "- Residuals (errors) should be **independent**.\n",
        "- If dealing with **time-series data**, errors may be correlated.\n",
        "\n",
        "üí° *Solution:*  \n",
        "- Use **Autoregressive models** or **Generalized Estimating Equations (GEE)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| **Assumption** | **Explanation** | **Solution if Violated** |\n",
        "|--------------|---------------|----------------------|\n",
        "| **Binary Dependent Variable** | Y must be 0 or 1 (for binary classification) | Use **multinomial logistic regression** for multi-class problems |\n",
        "| **Independence of Observations** | Each observation should be independent | Use **random sampling**, avoid duplicate records |\n",
        "| **Linearity of Log-Odds** | Log-odds of Y should have a linear relationship with X | Apply **polynomial features** or **transformations** |\n",
        "| **No Perfect Multicollinearity** | Features should not be highly correlated | Drop one feature, use **VIF**, or apply **PCA** |\n",
        "| **Large Sample Size** | Needs sufficient data for stable coefficients | Ensure **at least 10 samples per feature per class** |\n",
        "| **No Strong Outliers** | Outliers can distort the model | Remove, transform, or cap outliers |\n",
        "| **Independence of Errors** | No autocorrelation in errors | Use **time-series models** if data is sequential |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "- Logistic Regression is **flexible**, but these assumptions help ensure reliable predictions.\n",
        "- If assumptions are **violated**, alternative techniques like **Decision Trees, Random Forest, or Neural Networks** may be better.\n",
        "\n",
        "\n",
        "10 What are some alternatives to Logistic Regression for classification tasks?\n",
        "-### **Alternatives to Logistic Regression for Classification Tasks**  \n",
        "\n",
        "Logistic Regression is a great baseline model for classification, but **it has limitations**, especially when dealing with **nonlinear relationships, high-dimensional data, or imbalanced datasets**. Here are some **alternative classification models**:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Decision Trees** üå≥  \n",
        "‚úÖ **Best When**:  \n",
        "- The relationship between features and the target is **nonlinear**.  \n",
        "- Interpretability is important.  \n",
        "\n",
        "üîπ **Pros**:  \n",
        "‚úî Works well with **nonlinear data**.  \n",
        "‚úî Easy to **interpret and visualize**.  \n",
        "‚úî Handles **categorical and numerical features**.  \n",
        "\n",
        "üîπ **Cons**:  \n",
        "‚úò **Prone to overfitting** (unless pruned).  \n",
        "‚úò **Sensitive to noisy data**.  \n",
        "\n",
        "üìå *Example:* Used in medical diagnosis to decide if a patient has a disease based on symptoms.  \n",
        "\n",
        "---\n",
        "\n",
        "## **2. Random Forest** üå≤üå≤  \n",
        "‚úÖ **Best When**:  \n",
        "- You need a **robust model** that reduces overfitting.  \n",
        "- Your dataset is **high-dimensional** with complex relationships.  \n",
        "\n",
        "üîπ **Pros**:  \n",
        "‚úî **Handles missing data well**.  \n",
        "‚úî Reduces overfitting (ensemble of trees).  \n",
        "‚úî Works with **both classification & regression** tasks.  \n",
        "\n",
        "üîπ **Cons**:  \n",
        "‚úò **Slower for large datasets** compared to Logistic Regression.  \n",
        "‚úò Less **interpretable than a single Decision Tree**.  \n",
        "\n",
        "üìå *Example:* Used for **credit card fraud detection**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **3. Support Vector Machine (SVM)** üéØ  \n",
        "‚úÖ **Best When**:  \n",
        "- The data is **not linearly separable**.  \n",
        "- You need a **powerful classifier** for small datasets.  \n",
        "\n",
        "üîπ **Pros**:  \n",
        "‚úî Works well in **high-dimensional spaces**.  \n",
        "‚úî Handles **nonlinear classification** using **kernel tricks**.  \n",
        "\n",
        "üîπ **Cons**:  \n",
        "‚úò **Computationally expensive** for large datasets.  \n",
        "‚úò **Hard to interpret** compared to Logistic Regression.  \n",
        "\n",
        "üìå *Example:* Used in **image recognition** (face detection).  \n",
        "\n",
        "---\n",
        "\n",
        "## **4. K-Nearest Neighbors (KNN)** üë¨  \n",
        "‚úÖ **Best When**:  \n",
        "- You have a **small dataset**.  \n",
        "- The decision boundary is **complex but smooth**.  \n",
        "\n",
        "üîπ **Pros**:  \n",
        "‚úî **Simple and intuitive**.  \n",
        "‚úî **No training phase** (lazy learning).  \n",
        "\n",
        "üîπ **Cons**:  \n",
        "‚úò **Computationally expensive** for large datasets.  \n",
        "‚úò **Sensitive to irrelevant features & outliers**.  \n",
        "\n",
        "üìå *Example:* Used in **recommendation systems** (e.g., suggesting similar movies).  \n",
        "\n",
        "---\n",
        "\n",
        "## **5. Na√Øve Bayes** üé≤  \n",
        "‚úÖ **Best When**:  \n",
        "- The features are **independent** or nearly independent.  \n",
        "- You need a **fast model for text classification**.  \n",
        "\n",
        "üîπ **Pros**:  \n",
        "‚úî **Works well with small datasets**.  \n",
        "‚úî Great for **text classification (spam detection, sentiment analysis)**.  \n",
        "\n",
        "üîπ **Cons**:  \n",
        "‚úò **Assumes feature independence** (not always realistic).  \n",
        "‚úò **Not suitable for highly correlated features**.  \n",
        "\n",
        "üìå *Example:* Used in **email spam filtering**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **6. Gradient Boosting (XGBoost, LightGBM, CatBoost)** üöÄ  \n",
        "‚úÖ **Best When**:  \n",
        "- You need a **high-performance model**.  \n",
        "- The dataset has **complex feature interactions**.  \n",
        "\n",
        "üîπ **Pros**:  \n",
        "‚úî **Handles missing values well**.  \n",
        "‚úî **Great for structured/tabular data**.  \n",
        "‚úî **State-of-the-art accuracy in many Kaggle competitions**.  \n",
        "\n",
        "üîπ **Cons**:  \n",
        "‚úò **Takes longer to train** than simpler models.  \n",
        "‚úò **Hard to interpret** compared to Logistic Regression.  \n",
        "\n",
        "üìå *Example:* Used in **predicting customer churn in telecom companies**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **7. Artificial Neural Networks (ANNs) üß†**  \n",
        "‚úÖ **Best When**:  \n",
        "- You have a **large dataset** with complex relationships.  \n",
        "- Deep learning is required for tasks like **speech and image recognition**.  \n",
        "\n",
        "üîπ **Pros**:  \n",
        "‚úî **Handles extremely complex data patterns**.  \n",
        "‚úî **Scales well to large datasets**.  \n",
        "\n",
        "üîπ **Cons**:  \n",
        "‚úò **Requires large training data**.  \n",
        "‚úò **Difficult to interpret**.  \n",
        "\n",
        "üìå *Example:* Used in **self-driving car perception systems**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison Table**\n",
        "\n",
        "| **Algorithm**  | **Best For** | **Pros** | **Cons** |\n",
        "|--------------|------------|---------|---------|\n",
        "| **Logistic Regression** | Simple, linear classification | Fast, interpretable | Struggles with non-linearity |\n",
        "| **Decision Tree** | Nonlinear problems, easy interpretation | Simple, interpretable | Overfits without pruning |\n",
        "| **Random Forest** | Robust classification, reducing overfitting | Handles missing data | Slower for large datasets |\n",
        "| **SVM** | High-dimensional data, non-linearity | Effective for small data | Slow for large datasets |\n",
        "| **KNN** | Simple, memory-based classification | No training required | Slow for large datasets |\n",
        "| **Na√Øve Bayes** | Text classification (spam, sentiment) | Fast, works with small data | Assumes feature independence |\n",
        "| **Gradient Boosting (XGBoost, LightGBM, etc.)** | Complex patterns in structured data | High accuracy | Slow training |\n",
        "| **Neural Networks** | Deep learning, image/speech recognition | Handles complex data | Needs large dataset, hard to interpret |\n",
        "\n",
        "---\n",
        "\n",
        "### **Which One Should You Choose?**\n",
        "- **Use Logistic Regression** if you need a **simple, interpretable, and fast model**.\n",
        "- **Use Decision Trees or Random Forest** if your data is **nonlinear**.\n",
        "- **Use SVM** if you have **complex, small-sized datasets**.\n",
        "- **Use Na√Øve Bayes** for **text classification**.\n",
        "- **Use XGBoost or LightGBM** for **structured data & high accuracy**.\n",
        "- **Use Neural Networks** if you have **large datasets and deep learning requirements**.\n",
        "\n",
        "\n",
        "\n",
        "11 What are Classification Evaluation Metrics?\n",
        "-When evaluating a classification model, we use different performance metrics to measure how well the model predicts class labels. The choice of metric depends on the dataset, the problem type (balanced vs. imbalanced classes), and the cost of misclassification.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "12 How does class imbalance affect Logistic Regression?\n",
        "-Class imbalance occurs when one class in a dataset significantly outnumbers the other(s), such as fraud detection (99% non-fraud, 1% fraud) or medical diagnosis (95% healthy, 5% sick). When using Logistic Regression, class imbalance can lead to biased predictions, affecting performance and misleading evaluation metrics\n",
        "\n",
        "\n",
        "\n",
        "13 What is Hyperparameter Tuning in Logistic Regression?\n",
        "-Hyperparameter tuning is the process of optimizing the hyperparameters of a model to achieve the best performance. Unlike model parameters (e.g., weights & biases in Logistic Regression), hyperparameters are set before training and cannot be learned from data\n",
        "\n",
        "\n",
        "\n",
        "14 What are different solvers in Logistic Regression? Which one should be used?\n",
        "-### **Solvers in Logistic Regression & How to Choose the Right One**  \n",
        "\n",
        "In **Logistic Regression**, the solver is the **optimization algorithm** that minimizes the cost function (Log Loss). The choice of solver affects **speed, accuracy, and compatibility** with different regularization methods.\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ List of Logistic Regression Solvers in `sklearn`**\n",
        "| **Solver**    | **Supports L1?** | **Supports L2?** | **Supports ElasticNet?** | **Best For** | **Gradient-Based?** |\n",
        "|--------------|----------------|----------------|-------------------|------------|----------------|\n",
        "| **liblinear** | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No  | Small datasets | No (Coordinate Descent) |\n",
        "| **lbfgs** | ‚ùå No | ‚úÖ Yes | ‚ùå No  | Large datasets, multiclass | Yes |\n",
        "| **sag** | ‚ùå No | ‚úÖ Yes | ‚ùå No  | Large datasets, online learning | Yes |\n",
        "| **saga** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes | Large datasets, supports all penalties | Yes |\n",
        "| **newton-cg** | ‚ùå No | ‚úÖ Yes | ‚ùå No  | Large datasets, multiclass | Yes |\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ When to Use Each Solver?**\n",
        "### **1Ô∏è‚É£ `liblinear` (Best for Small Datasets)**\n",
        "- Uses **Coordinate Descent** instead of gradients.\n",
        "- Supports both **L1 (Lasso) and L2 (Ridge)** regularization.\n",
        "- **Slower** for large datasets.\n",
        "\n",
        "‚úÖ **Use When:**  \n",
        "- **Dataset is small (few samples & features).**  \n",
        "- **Sparse data (lots of zeros).**  \n",
        "- **Need L1 (Lasso) for feature selection.**\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ `lbfgs` (Best for Large Datasets & Multiclass)**\n",
        "- Uses **BFGS Approximation** (efficient for gradient-based optimization).\n",
        "- Supports **L2 regularization** only.\n",
        "- Can handle **multiclass problems (Softmax Regression)**.\n",
        "\n",
        "‚úÖ **Use When:**  \n",
        "- **Dataset is large.**  \n",
        "- **Need multi-class classification (`multi_class='multinomial'`).**  \n",
        "- **Don't need L1 (Lasso) or ElasticNet.**\n",
        "\n",
        "---\n",
        "\n",
        "### **3Ô∏è‚É£ `sag` (Best for Large Datasets with L2)**\n",
        "- Uses **Stochastic Average Gradient Descent (SAG)**.\n",
        "- Supports **only L2 regularization**.\n",
        "- Works well with **very large datasets** but needs **scaled features**.\n",
        "\n",
        "‚úÖ **Use When:**  \n",
        "- **Dataset is very large.**  \n",
        "- **Only need L2 regularization.**  \n",
        "- **Need online learning (stochastic updates).**  \n",
        "\n",
        "‚ö† **Don't Use If:**  \n",
        "- Dataset is **small** (overhead is high).  \n",
        "- Need **L1 or ElasticNet**.\n",
        "\n",
        "---\n",
        "\n",
        "### **4Ô∏è‚É£ `saga` (Best for Large Datasets & All Regularization Types)**\n",
        "- **Supports L1, L2, and ElasticNet** regularization.\n",
        "- Works for **large datasets** with stochastic updates.\n",
        "- Suitable for **both binary & multiclass classification**.\n",
        "\n",
        "‚úÖ **Use When:**  \n",
        "- **Dataset is large & high-dimensional.**  \n",
        "- **Need L1, L2, or ElasticNet.**  \n",
        "- **Sparse data (many zeros).**  \n",
        "\n",
        "üöÄ **Best choice for general use with big data**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5Ô∏è‚É£ `newton-cg` (Best for Large Datasets & Multiclass)**\n",
        "- Uses **Newton‚Äôs Conjugate Gradient** method (2nd-order optimization).\n",
        "- **More memory-intensive** but faster convergence than gradient-based solvers.\n",
        "- Supports **only L2 regularization**.\n",
        "\n",
        "‚úÖ **Use When:**  \n",
        "- **Dataset is large & complex.**  \n",
        "- **Need multiclass classification (`multi_class='multinomial'`).**  \n",
        "- **Prefer second-order optimization over gradient descent.**  \n",
        "\n",
        "‚ö† **Don't Use If:**  \n",
        "- Need **L1 or ElasticNet**.  \n",
        "- Memory is limited (requires more memory than `lbfgs`).  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Summary: Which Solver Should You Use?**\n",
        "| **Scenario** | **Best Solver** |\n",
        "|------------|--------------|\n",
        "| **Small dataset** | `liblinear` |\n",
        "| **Large dataset** | `lbfgs`, `sag`, or `saga` |\n",
        "| **L1 Regularization (Feature Selection)** | `liblinear`, `saga` |\n",
        "| **L2 Regularization** | `lbfgs`, `sag`, `saga`, `newton-cg`, `liblinear` |\n",
        "| **ElasticNet Regularization** | `saga` |\n",
        "| **Multiclass Classification** | `lbfgs`, `newton-cg`, `saga` |\n",
        "| **Sparse Data (Many Zeros)** | `liblinear`, `saga` |\n",
        "\n",
        "---\n",
        "\n",
        "### **üõ† Example: Choosing the Right Solver in Python**\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Example: Large dataset with L2 regularization\n",
        "model = LogisticRegression(solver='lbfgs', penalty='l2', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "15 How is Logistic Regression extended for multiclass classification?\n",
        "-### **Extending Logistic Regression for Multiclass Classification**  \n",
        "\n",
        "Logistic Regression is inherently a **binary classifier**, but it can be extended to handle **multiclass classification** using two main approaches:  \n",
        "\n",
        "1Ô∏è‚É£ **One-vs-Rest (OvR) / One-vs-All (OvA)**  \n",
        "2Ô∏è‚É£ **Multinomial Logistic Regression (Softmax Regression)**  \n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ One-vs-Rest (OvR) / One-vs-All (OvA)**\n",
        "- The **OvR approach** trains **multiple binary Logistic Regression models**.\n",
        "- For **K classes**, it trains **K separate models**, each classifying **one class vs. the rest**.\n",
        "- The final prediction is based on the class with the **highest probability**.\n",
        "\n",
        "üîπ **Example (3 classes: A, B, C)**  \n",
        "- **Model 1:** Classify **A vs. (B, C)**  \n",
        "- **Model 2:** Classify **B vs. (A, C)**  \n",
        "- **Model 3:** Classify **C vs. (A, B)**  \n",
        "- For a new input, all models predict probabilities, and the class with the **highest probability is chosen**.\n",
        "\n",
        "‚úÖ **Pros**:  \n",
        "- Simple, works well with any binary classifier.  \n",
        "- Computationally efficient for **many classes**.  \n",
        "\n",
        "‚ùå **Cons**:  \n",
        "- Can be **inconsistent** (two models may assign similar probabilities to different classes).  \n",
        "- Requires **K separate models**, increasing training time.  \n",
        "\n",
        "üìå **How to Use in `sklearn`**:\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train Logistic Regression using One-vs-Rest\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs')  # Default setting\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Multinomial Logistic Regression (Softmax Regression)**\n",
        "- Instead of training multiple models, **Softmax Regression** directly estimates probabilities for all classes **simultaneously**.\n",
        "- Uses the **Softmax function** to assign a probability to each class:\n",
        "  \n",
        "  \\[\n",
        "  P(y = k | X) = \\frac{e^{(\\theta_k \\cdot X)}}{\\sum_{j=1}^{K} e^{(\\theta_j \\cdot X)}}\n",
        "  \\]\n",
        "\n",
        "  - **Numerator:** Computes the exponentiated linear score for class \\( k \\).  \n",
        "  - **Denominator:** Normalizes by summing exponentiated scores across all classes.  \n",
        "  - The class with the **highest probability** is the prediction.\n",
        "\n",
        "‚úÖ **Pros**:  \n",
        "- More **consistent** and accurate than OvR.  \n",
        "- Works better with **balanced classes**.  \n",
        "\n",
        "‚ùå **Cons**:  \n",
        "- Computationally **expensive for large datasets**.  \n",
        "- May be **unstable** if classes overlap significantly.  \n",
        "\n",
        "üìå **How to Use in `sklearn`**:\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train Logistic Regression using Multinomial (Softmax)\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')  \n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **üÜö OvR vs. Multinomial: Which One to Use?**\n",
        "| **Criteria** | **OvR (One-vs-Rest)** | **Multinomial (Softmax)** |\n",
        "|-------------|-----------------|-----------------|\n",
        "| **Number of Models** | K separate models | Single model |\n",
        "| **Computational Cost** | Lower | Higher |\n",
        "| **Prediction Consistency** | Can be inconsistent | More consistent |\n",
        "| **Best for Small Data?** | ‚úÖ Yes | ‚ùå No |\n",
        "| **Best for Large Data?** | ‚ùå No | ‚úÖ Yes |\n",
        "| **Sparse Data?** | ‚úÖ Yes | ‚ùå No |\n",
        "| **Supports all Solvers?** | ‚úÖ Yes | ‚ùå No (Only `lbfgs`, `newton-cg`, `saga`) |\n",
        "\n",
        "üìå **Recommendation**:  \n",
        "- **Use OvR** for **small datasets** or when **training speed matters**.  \n",
        "- **Use Multinomial (Softmax)** for **large datasets** when accuracy is the priority.  \n",
        "\n",
        "\n",
        "\n",
        "16 What are the advantages and disadvantages of Logistic Regression?\n",
        "-### **Advantages and Disadvantages of Logistic Regression**  \n",
        "\n",
        "Logistic Regression is a simple and effective classification algorithm, but it also has some limitations. Let‚Äôs break it down:  \n",
        "\n",
        "---\n",
        "\n",
        "## **‚úÖ Advantages of Logistic Regression**  \n",
        "\n",
        "### **1Ô∏è‚É£ Simplicity & Interpretability**\n",
        "- **Easy to understand and interpret** compared to complex models like neural networks.  \n",
        "- The **coefficients** can be analyzed to understand **feature importance**.  \n",
        "\n",
        "### **2Ô∏è‚É£ Works Well for Linearly Separable Data**\n",
        "- If the classes can be separated by a straight line (in 2D) or a hyperplane (in higher dimensions), **Logistic Regression performs well**.  \n",
        "\n",
        "### **3Ô∏è‚É£ Probabilistic Predictions**\n",
        "- Outputs **probabilities** instead of just class labels, making it useful for **risk assessment and decision-making**.  \n",
        "- Example: Predicting the **probability of a customer churning** instead of just saying \"yes\" or \"no\".  \n",
        "\n",
        "### **4Ô∏è‚É£ Computationally Efficient**\n",
        "- **Faster to train** than complex models like Random Forests, SVMs, or Neural Networks.  \n",
        "- Works well on **small to medium-sized datasets**.  \n",
        "\n",
        "### **5Ô∏è‚É£ Handles Multicollinearity with Regularization**\n",
        "- Adding **L1 (Lasso) or L2 (Ridge) regularization** can reduce the impact of highly correlated features.  \n",
        "- Helps prevent **overfitting**.  \n",
        "\n",
        "### **6Ô∏è‚É£ Works for Multiclass Classification**\n",
        "- Can be extended to **multiclass problems** using **One-vs-Rest (OvR)** or **Softmax Regression (Multinomial Logistic Regression)**.  \n",
        "\n",
        "### **7Ô∏è‚É£ Feature Selection with L1 Regularization**\n",
        "- Lasso (`L1`) regression can shrink some feature coefficients **to zero**, effectively selecting only the most important features.  \n",
        "\n",
        "---\n",
        "\n",
        "## **‚ùå Disadvantages of Logistic Regression**  \n",
        "\n",
        "### **1Ô∏è‚É£ Assumes Linear Decision Boundary**\n",
        "- Logistic Regression assumes that **features have a linear relationship** with the log-odds of the target variable.  \n",
        "- **Doesn‚Äôt work well for complex, non-linear relationships** unless feature transformations are applied (e.g., polynomial features).  \n",
        "- üöÄ **Solution**: Use **Polynomial Logistic Regression** or switch to a non-linear model like **Decision Trees or Neural Networks**.  \n",
        "\n",
        "### **2Ô∏è‚É£ Sensitive to Outliers**\n",
        "- Outliers can **significantly impact the model**, as they can distort the decision boundary.  \n",
        "- üöÄ **Solution**:  \n",
        "  - Use **Robust Scaling (e.g., Median Absolute Deviation, IQR Scaling)**.  \n",
        "  - Use **L1 regularization** to reduce the effect of outliers.  \n",
        "\n",
        "### **3Ô∏è‚É£ Requires Feature Engineering & Scaling**\n",
        "- **Assumes features are independent** and should be properly scaled (**standardization or normalization**).  \n",
        "- üöÄ **Solution**:  \n",
        "  - Apply **MinMax Scaling or Standardization (Z-score scaling)**.  \n",
        "  - Perform **feature engineering** to handle interactions.  \n",
        "\n",
        "### **4Ô∏è‚É£ Struggles with Highly Imbalanced Data**\n",
        "- If one class dominates the dataset, Logistic Regression tends to **predict the majority class** most of the time.  \n",
        "- üöÄ **Solution**:  \n",
        "  - Use **class weighting** (`class_weight='balanced'` in `sklearn`).  \n",
        "  - Try **oversampling (SMOTE) or undersampling techniques**.  \n",
        "\n",
        "### **5Ô∏è‚É£ Doesn‚Äôt Work Well with High-Dimensional Data**\n",
        "- In datasets with **many features but few samples**, Logistic Regression may overfit.  \n",
        "- üöÄ **Solution**:  \n",
        "  - Use **L1 (Lasso) Regularization** for **feature selection**.  \n",
        "  - Switch to **Tree-based models (Random Forest, XGBoost)**.  \n",
        "\n",
        "### **6Ô∏è‚É£ Requires a Large Dataset for Good Generalization**\n",
        "- **Needs a sufficient number of training examples** to generalize well.  \n",
        "- If the dataset is **too small**, it may not capture the underlying pattern.  \n",
        "- üöÄ **Solution**: Use **Bayesian Logistic Regression** for small datasets.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üìå Summary: When to Use Logistic Regression?**  \n",
        "\n",
        "| **Scenario** | **Should You Use Logistic Regression?** |\n",
        "|-------------|---------------------------------|\n",
        "| **Binary classification** | ‚úÖ Yes |\n",
        "| **Linear decision boundary** | ‚úÖ Yes |\n",
        "| **Interpretability is important** | ‚úÖ Yes |\n",
        "| **Feature selection needed (L1 regularization)** | ‚úÖ Yes |\n",
        "| **Multicollinearity exists (L2 regularization)** | ‚úÖ Yes |\n",
        "| **Multiclass classification** | ‚úÖ (Use Softmax Regression) |\n",
        "| **Highly non-linear relationships** | ‚ùå No (Use Decision Trees, Neural Networks) |\n",
        "| **High-dimensional sparse data** | ‚ùå No (Use SVM, Tree-based models) |\n",
        "| **Imbalanced dataset** | ‚ùå No (Use SMOTE, class weights) |\n",
        "\n",
        "\n",
        "\n",
        "17  What are some use cases of Logistic Regression?\n",
        "-### **Use Cases of Logistic Regression** üöÄ  \n",
        "\n",
        "Logistic Regression is widely used for **classification tasks** where the goal is to predict **binary or multiclass outcomes**. Below are some common real-world applications:  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 1. Medical Diagnosis & Healthcare** üè•  \n",
        "‚úî **Disease Prediction**:  \n",
        "   - Predict whether a patient has **diabetes**, **heart disease**, or **cancer** based on medical parameters.  \n",
        "   - Example: **Diabetes Prediction** using BMI, glucose level, and blood pressure.  \n",
        "\n",
        "‚úî **COVID-19 Risk Assessment**:  \n",
        "   - Classifying patients into **high-risk** vs. **low-risk** categories based on symptoms and test results.  \n",
        "\n",
        "‚úî **Survival Analysis**:  \n",
        "   - Predicting the **likelihood of patient survival** after treatment (e.g., cancer prognosis).  \n",
        "\n",
        "‚úî **Mental Health Detection**:  \n",
        "   - Identifying individuals with **depression or anxiety** based on survey responses and behavioral data.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 2. Financial & Banking Sector** üí∞  \n",
        "‚úî **Credit Scoring & Loan Default Prediction**:  \n",
        "   - Predict if a customer will **default on a loan** based on credit history, income, and spending habits.  \n",
        "\n",
        "‚úî **Fraud Detection**:  \n",
        "   - Classifying transactions as **fraudulent or legitimate**.  \n",
        "   - Used in **credit card fraud detection** üö®.  \n",
        "\n",
        "‚úî **Customer Churn Prediction**:  \n",
        "   - Predict whether a customer will **leave a bank or financial service**.  \n",
        "   - Helps in **retention strategies**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 3. Marketing & Customer Analytics** üìä  \n",
        "‚úî **Customer Purchase Prediction**:  \n",
        "   - Will a customer **buy a product** or not?  \n",
        "   - Used in **advertising campaigns** to **predict ad clicks** (Click-Through Rate Prediction).  \n",
        "\n",
        "‚úî **Lead Scoring**:  \n",
        "   - Classifying potential customers into **\"likely to convert\" vs. \"unlikely to convert\"**.  \n",
        "\n",
        "‚úî **Email Spam Detection**:  \n",
        "   - Classifying emails as **spam or not spam** üìß.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 4. Human Resources & Hiring** üë©‚Äçüíº  \n",
        "‚úî **Employee Attrition Prediction**:  \n",
        "   - Will an employee **quit or stay** in a company?  \n",
        "   - Helps HR teams take **proactive actions**.  \n",
        "\n",
        "‚úî **Candidate Selection**:  \n",
        "   - Predicting if a job candidate will be **hired or rejected** based on qualifications and interview performance.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 5. Social Media & Sentiment Analysis** üì±  \n",
        "‚úî **Fake News Detection**:  \n",
        "   - Classifying news articles as **real or fake**.  \n",
        "\n",
        "‚úî **Sentiment Analysis**:  \n",
        "   - Predicting whether a **tweet, review, or comment** is **positive or negative**.  \n",
        "\n",
        "‚úî **Toxic Comment Detection**:  \n",
        "   - Identifying **hateful, offensive, or inappropriate content** on platforms like YouTube, Twitter, etc.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 6. Manufacturing & Quality Control** üè≠  \n",
        "‚úî **Defective Product Detection**:  \n",
        "   - Classifying products as **\"defective\" or \"non-defective\"** in quality control.  \n",
        "\n",
        "‚úî **Machine Failure Prediction**:  \n",
        "   - Predicting **whether a machine will fail** based on sensor data.  \n",
        "   - Used in **preventive maintenance**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 7. Transportation & Logistics** üöó  \n",
        "‚úî **Accident Severity Prediction**:  \n",
        "   - Predicting whether an accident will be **minor or severe** based on road conditions, weather, and vehicle data.  \n",
        "\n",
        "‚úî **Customer Satisfaction Classification**:  \n",
        "   - Classifying **customer feedback** as **satisfied or dissatisfied** for ride-sharing companies like Uber, Lyft, etc.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Summary: When to Use Logistic Regression?**  \n",
        "| **Use Case** | **Logistic Regression?** |\n",
        "|-------------|------------------|\n",
        "| **Binary classification problems** | ‚úÖ Yes |\n",
        "| **Need probability estimates** | ‚úÖ Yes |\n",
        "| **Small to medium-sized datasets** | ‚úÖ Yes |\n",
        "| **Interpretable model required** | ‚úÖ Yes |\n",
        "| **Highly non-linear data** | ‚ùå No (Use Decision Trees, Neural Networks) |\n",
        "| **High-dimensional sparse data** | ‚ùå No (Use SVM, Tree-based models) |\n",
        "\n",
        "\n",
        "\n",
        "18 What is the difference between Softmax Regression and Logistic Regression?\n",
        "-### **Softmax Regression vs. Logistic Regression** üöÄ  \n",
        "\n",
        "Both **Logistic Regression** and **Softmax Regression** are used for classification tasks, but they differ in the number of classes they handle and how they make predictions.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 1. Logistic Regression**  \n",
        "‚úî **Used for:** **Binary Classification** (2 classes: e.g., Yes/No, 0/1)  \n",
        "‚úî **Output:** **Single probability value** (probability of belonging to class 1)  \n",
        "‚úî **Activation Function:** **Sigmoid Function**  \n",
        "‚úî **Decision Rule:**  \n",
        "   - If \\( P(y=1 | X) > 0.5 \\) ‚Üí Predict Class 1  \n",
        "   - If \\( P(y=1 | X) \\leq 0.5 \\) ‚Üí Predict Class 0  \n",
        "‚úî **Formula:**  \n",
        "   \\[\n",
        "   P(y=1 | X) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 X_1 + \\dots + \\theta_n X_n)}}\n",
        "   \\]  \n",
        "‚úî **Example:**  \n",
        "   - Predict if a customer will **buy a product (Yes/No)**.  \n",
        "   - Predict if an email is **spam or not spam**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 2. Softmax Regression (Multinomial Logistic Regression)**  \n",
        "‚úî **Used for:** **Multiclass Classification** (3+ classes: e.g., A/B/C, Dog/Cat/Rabbit)  \n",
        "‚úî **Output:** **Probability distribution across multiple classes**  \n",
        "‚úî **Activation Function:** **Softmax Function**  \n",
        "‚úî **Decision Rule:**  \n",
        "   - The class with the **highest probability** is chosen.  \n",
        "‚úî **Formula:**  \n",
        "   \\[\n",
        "   P(y = k | X) = \\frac{e^{(\\theta_k \\cdot X)}}{\\sum_{j=1}^{K} e^{(\\theta_j \\cdot X)}}\n",
        "   \\]  \n",
        "‚úî **Example:**  \n",
        "   - Predict **which genre a movie belongs to** (Comedy, Action, Drama).  \n",
        "   - Classify handwritten digits **(0-9 in MNIST dataset)**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Key Differences: Logistic vs. Softmax Regression**\n",
        "| Feature | Logistic Regression | Softmax Regression |\n",
        "|---------|---------------------|---------------------|\n",
        "| **Type of Classification** | Binary (2 classes) | Multiclass (3+ classes) |\n",
        "| **Activation Function** | Sigmoid | Softmax |\n",
        "| **Output** | Single probability (for class 1) | Probability distribution across all classes |\n",
        "| **Formula** | \\( P(y=1 | X) = \\frac{1}{1 + e^{-\\theta X}} \\) | \\( P(y=k | X) = \\frac{e^{(\\theta_k X)}}{\\sum_{j=1}^{K} e^{(\\theta_j X)}} \\) |\n",
        "| **Decision Rule** | \\( P(y=1 | X) > 0.5 \\) ‚Üí Class 1, else Class 0 | Pick the class with the highest probability |\n",
        "| **Use Case** | Spam detection, Loan approval | Handwritten digit recognition, Image classification |\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ When to Use Which?**\n",
        "- **Use Logistic Regression** for **binary classification problems**.  \n",
        "- **Use Softmax Regression** when there are **more than two classes** and you need **probability scores for each class**.  \n",
        "\n",
        "\n",
        "\n",
        "19 How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "-### **Choosing Between One-vs-Rest (OvR) and Softmax for Multiclass Classification** üöÄ  \n",
        "\n",
        "When extending **Logistic Regression** to **multiclass problems**, we have two main approaches:  \n",
        "1Ô∏è‚É£ **One-vs-Rest (OvR) / One-vs-All (OvA)**  \n",
        "2Ô∏è‚É£ **Softmax Regression (Multinomial Logistic Regression)**  \n",
        "\n",
        "Both methods work, but choosing the right one depends on **dataset size, performance needs, and interpretability**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 1. One-vs-Rest (OvR) / One-vs-All (OvA)**\n",
        "‚úî **How it works:**  \n",
        "   - Trains **K separate binary classifiers** (for K classes).  \n",
        "   - Each classifier predicts **one class vs. the rest**.  \n",
        "   - The class with the **highest probability wins**.  \n",
        "\n",
        "‚úî **Pros:**  \n",
        "‚úÖ Simple and easy to implement.  \n",
        "‚úÖ Works well even with **small datasets**.  \n",
        "‚úÖ Can use **any binary classifier**, not just Logistic Regression.  \n",
        "‚úÖ **Computationally cheaper** for many classes when using simple classifiers.  \n",
        "\n",
        "‚úî **Cons:**  \n",
        "‚ùå Training multiple models can be slow if **K is large**.  \n",
        "‚ùå Predictions may be **inconsistent** (one model might predict high probabilities for two classes).  \n",
        "\n",
        "üìå **When to Use OvR?**  \n",
        "‚úî When the number of classes is **small to moderate** (e.g., K < 10).  \n",
        "‚úî When the dataset is **small**, and we want to avoid overfitting.  \n",
        "‚úî When we need a **simpler model with interpretability**.  \n",
        "\n",
        "üìå **Example in `sklearn`**:\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train One-vs-Rest Logistic Regression\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 2. Softmax Regression (Multinomial Logistic Regression)**\n",
        "‚úî **How it works:**  \n",
        "   - Uses a **single model** instead of multiple binary models.  \n",
        "   - The **Softmax function** computes probabilities for all classes simultaneously.  \n",
        "   - The class with the **highest probability wins**.  \n",
        "\n",
        "‚úî **Pros:**  \n",
        "‚úÖ More **consistent** predictions than OvR.  \n",
        "‚úÖ **Better performance** on **large datasets**.  \n",
        "‚úÖ The probabilities are **mutually exclusive**, meaning total probability sums to 1.  \n",
        "\n",
        "‚úî **Cons:**  \n",
        "‚ùå Computationally **expensive** for large K (requires matrix operations).  \n",
        "‚ùå Only works well when **all classes have enough training data**.  \n",
        "\n",
        "üìå **When to Use Softmax?**  \n",
        "‚úî When **K is large** (e.g., K > 10).  \n",
        "‚úî When we have a **large dataset** with sufficient training examples per class.  \n",
        "‚úî When we need **probabilities that sum to 1** for better decision-making.  \n",
        "\n",
        "üìå **Example in `sklearn`**:\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train Multinomial (Softmax) Logistic Regression\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Comparison: OvR vs. Softmax**\n",
        "| **Feature** | **OvR (One-vs-Rest)** | **Softmax (Multinomial)** |\n",
        "|------------|-----------------|------------------|\n",
        "| **Number of Models** | K binary classifiers | 1 single model |\n",
        "| **Computational Cost** | Lower for small K | Higher for large K |\n",
        "| **Consistency** | May give inconsistent probabilities | More consistent predictions |\n",
        "| **Probability Interpretation** | Not guaranteed to sum to 1 | Always sums to 1 |\n",
        "| **Best for Small Datasets?** | ‚úÖ Yes | ‚ùå No |\n",
        "| **Best for Large Datasets?** | ‚ùå No | ‚úÖ Yes |\n",
        "| **Best for Many Classes?** | ‚ùå No | ‚úÖ Yes |\n",
        "| **Flexibility with Other Models?** | ‚úÖ Can use any binary classifier | ‚ùå Only Logistic Regression |\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Final Recommendation:**\n",
        "- **Use OvR** when **K is small** or if the dataset is **small and imbalanced**.  \n",
        "- **Use Softmax** when **K is large** and we need a **more stable probabilistic output**.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "20 How do we interpret coefficients in Logistic Regression?\n",
        "-### **Interpreting Coefficients in Logistic Regression** üîç  \n",
        "\n",
        "In **Logistic Regression**, the model predicts the **log-odds** of the outcome. The coefficients (\\(\\beta\\)) represent how each feature influences the probability of the outcome. However, they are not directly interpretable like in **Linear Regression**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 1. Understanding the Logistic Regression Equation**  \n",
        "Logistic Regression models the probability as:  \n",
        "\n",
        "\\[\n",
        "P(y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n)}}\n",
        "\\]  \n",
        "\n",
        "Taking the **log-odds (logit function):**  \n",
        "\n",
        "\\[\n",
        "\\log\\left(\\frac{P(y=1)}{1 - P(y=1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n\n",
        "\\]  \n",
        "\n",
        "Each coefficient **\\(\\beta_j\\)** represents the change in the **log-odds** of the outcome **for a one-unit increase in \\(X_j\\)**, keeping other variables constant.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 2. Interpreting Coefficients**  \n",
        "\n",
        "### **(A) Direct Interpretation: Log-Odds**  \n",
        "- A **positive coefficient** (\\(\\beta_j > 0\\)) ‚Üí **Increases** log-odds ‚Üí **Increases probability** of \\( y=1 \\).  \n",
        "- A **negative coefficient** (\\(\\beta_j < 0\\)) ‚Üí **Decreases** log-odds ‚Üí **Decreases probability** of \\( y=1 \\).  \n",
        "\n",
        "üí° **Example:** If \\(\\beta_1 = 0.8\\), it means that for every **1-unit increase** in \\(X_1\\), the log-odds of \\(y=1\\) **increase by 0.8**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **(B) Converting to Odds Ratio (Exponentiation)**\n",
        "To make the coefficients more interpretable, we **exponentiate them**:  \n",
        "\n",
        "\\[\n",
        "\\text{Odds Ratio} = e^{\\beta_j}\n",
        "\\]  \n",
        "\n",
        "- If **\\( e^{\\beta_j} > 1 \\)** ‚Üí The probability of \\( y=1 \\) **increases** with \\( X_j \\).  \n",
        "- If **\\( e^{\\beta_j} < 1 \\)** ‚Üí The probability of \\( y=1 \\) **decreases** with \\( X_j \\).  \n",
        "- If **\\( e^{\\beta_j} = 1 \\)** ‚Üí \\( X_j \\) has **no effect** on \\( y \\).  \n",
        "\n",
        "üí° **Example:**  \n",
        "If \\(\\beta_1 = 0.8\\), then:  \n",
        "\n",
        "\\[\n",
        "e^{0.8} \\approx 2.23\n",
        "\\]  \n",
        "\n",
        "**Interpretation:** A **1-unit increase** in \\(X_1\\) **multiplies the odds of \\( y=1 \\) by 2.23**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **(C) Percentage Change Interpretation**\n",
        "A more intuitive way to express the impact is:  \n",
        "\n",
        "\\[\n",
        "(\\text{Odds Ratio} - 1) \\times 100\\%\n",
        "\\]  \n",
        "\n",
        "üí° **Example:** If **\\( e^{\\beta_1} = 2.23 \\)**, then:  \n",
        "\\[\n",
        "(2.23 - 1) \\times 100 = 123\\%\n",
        "\\]  \n",
        "\n",
        "**Interpretation:** A **1-unit increase** in \\(X_1\\) increases the **odds** of \\( y=1 \\) by **123%**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 3. Interpreting Categorical Variables**  \n",
        "For **binary categorical variables (e.g., Gender: Male = 0, Female = 1):**  \n",
        "- If **\\(\\beta = 0.5\\)** for **Female**, then **being Female increases the log-odds of \\( y=1 \\) by 0.5**.  \n",
        "- Converting to odds ratio:  \n",
        "  \\[\n",
        "  e^{0.5} \\approx 1.65\n",
        "  \\]\n",
        "  **Interpretation:** Being Female **increases the odds of \\( y=1 \\) by 65%** compared to Male.  \n",
        "\n",
        "For **multi-category variables (e.g., Education: High School, College, Graduate)**, we use **dummy encoding**, and each level is compared to a reference category.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 4. Example in Python (`sklearn`)**  \n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Sample dataset\n",
        "data = pd.DataFrame({\n",
        "    'Age': [25, 30, 35, 40, 45, 50, 55, 60],\n",
        "    'Income': [30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000],\n",
        "    'Purchased': [0, 0, 0, 1, 1, 1, 1, 1]  # Binary target (0 = No, 1 = Yes)\n",
        "})\n",
        "\n",
        "X = data[['Age', 'Income']]\n",
        "y = data['Purchased']\n",
        "\n",
        "# Fit logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get coefficients\n",
        "coefs = pd.DataFrame({'Feature': X.columns, 'Coefficient': model.coef_[0]})\n",
        "coefs['Odds Ratio'] = np.exp(coefs['Coefficient'])\n",
        "\n",
        "print(coefs)\n",
        "```\n",
        "### **Sample Output:**\n",
        "| Feature | Coefficient | Odds Ratio |\n",
        "|---------|-------------|------------|\n",
        "| Age | 0.15 | 1.16 |\n",
        "| Income | 0.00002 | 1.00002 |\n",
        "\n",
        "**Interpretation:**\n",
        "- **Age:** For **each additional year**, the odds of purchase **increase by 16%**.  \n",
        "- **Income:** For **each extra dollar of income**, the odds **increase slightly (almost negligible change)**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ 5. Key Takeaways**\n",
        "‚úÖ **Logistic Regression coefficients affect log-odds, not probabilities directly.**  \n",
        "‚úÖ **Exponentiate coefficients** to get an **odds ratio** for easier interpretation.  \n",
        "‚úÖ **Positive coefficients** increase the odds of the event happening; **negative coefficients** decrease them.  \n",
        "‚úÖ **Categorical variables** need dummy encoding for interpretation.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I9MENvDiCb_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                 #Practical#\n",
        "\n",
        "\n",
        "1 Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy.\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset (Example: Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and target\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (recommended for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Model Accuracy: 0.97\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00        10\n",
        "           1       1.00      0.91      0.95        11\n",
        "           2       0.92      1.00      0.96         9\n",
        "\n",
        "    accuracy                           0.97        30\n",
        "   macro avg       0.97      0.97      0.97        30\n",
        "weighted avg       0.97      0.97      0.97        30\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2 Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset (Example: Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and target\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model with L1 Regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200, C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "OUTPUT:\n",
        "\n",
        "Model Accuracy: 0.97\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00        10\n",
        "           1       1.00      0.91      0.95        11\n",
        "           2       0.92      1.00      0.96         9\n",
        "\n",
        "    accuracy                           0.97        30\n",
        "   macro avg       0.97      0.97      0.97        30\n",
        "weighted avg       0.97      0.97      0.97        30\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3 Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset (Example: Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and target\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model with L2 Regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"\\nModel Coefficients (L2 Regularization):\")\n",
        "for feature, coef in zip(iris.feature_names, model.coef_.T):\n",
        "    print(f\"{feature}: {coef}\")\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Model Accuracy: 0.97\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00        10\n",
        "           1       1.00      0.91      0.95        11\n",
        "           2       0.92      1.00      0.96         9\n",
        "\n",
        "    accuracy                           0.97        30\n",
        "   macro avg       0.97      0.97      0.97        30\n",
        "weighted avg       0.97      0.97      0.97        30\n",
        "\n",
        "Model Coefficients (L2 Regularization):\n",
        "sepal length (cm): [-0.31338606  0.00304094  0.41680946]\n",
        "sepal width (cm): [ 0.7150656  -0.5072757  -0.78585371]\n",
        "petal length (cm): [-0.92179789  0.36307052  1.0121701 ]\n",
        "petal width (cm): [-0.83763588 -0.85963563  1.64654249]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4 Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset (Example: Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and target\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model with Elastic Net Regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200, C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"\\nModel Coefficients (Elastic Net Regularization):\")\n",
        "for feature, coef in zip(iris.feature_names, model.coef_.T):\n",
        "    print(f\"{feature}: {coef}\")\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "-Model Accuracy: 0.97\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00        10\n",
        "           1       1.00      0.91      0.95        11\n",
        "           2       0.92      1.00      0.96         9\n",
        "\n",
        "    accuracy                           0.97        30\n",
        "   macro avg       0.97      0.97      0.97        30\n",
        "weighted avg       0.97      0.97      0.97        30\n",
        "\n",
        "Model Coefficients (Elastic Net Regularization):\n",
        "sepal length (cm): [-0.312  0.002  0.415]\n",
        "sepal width (cm): [ 0.713 -0.506 -0.784]\n",
        "petal length (cm): [-0.920  0.362  1.010]\n",
        "petal width (cm): [-0.835 -0.858  1.644]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5 Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset (Example: Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and target\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model with One-vs-Rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Model Accuracy: 0.97\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00        10\n",
        "           1       1.00      0.91      0.95        11\n",
        "           2       0.92      1.00      0.96         9\n",
        "\n",
        "    accuracy                           0.97        30\n",
        "   macro avg       0.97      0.97      0.97        30\n",
        "weighted avg       0.97      0.97      0.97        30\n",
        "\n",
        "\n",
        "\n",
        "6 Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Example: Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and target\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(solver='saga', max_iter=500)\n",
        "\n",
        "# Define the hyperparameters grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],  # Regularization strength (inverse of lambda)\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],  # Different types of regularization\n",
        "    'l1_ratio': [0.2, 0.5, 0.8]  # Only used for Elastic Net\n",
        "}\n",
        "\n",
        "# Perform Grid Search with Cross Validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions with best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy with Best Parameters: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Best Parameters: {'C': 1, 'penalty': 'l2'}\n",
        "Model Accuracy with Best Parameters: 0.97\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7 Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Example: Iris dataset)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and target\n",
        "\n",
        "# Standardize features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Define Stratified K-Fold Cross-Validation\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500, solver='lbfgs', multi_class='ovr')\n",
        "\n",
        "# Perform Cross-Validation\n",
        "cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(\"Cross-Validation Accuracies for each fold:\", cv_scores)\n",
        "print(f\"Average Accuracy: {cv_scores.mean():.2f}\")\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Cross-Validation Accuracies for each fold: [0.97 1.00 0.97 0.97 0.97]\n",
        "Average Accuracy: 0.98\n",
        "\n",
        "\n",
        "\n",
        "8 Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy\n",
        "-import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset from CSV file (Replace 'your_dataset.csv' with the actual file path)\n",
        "df = pd.read_csv(\"your_dataset.csv\")\n",
        "\n",
        "# Assume the last column is the target (adjust based on your dataset)\n",
        "X = df.iloc[:, :-1].values  # Features (all columns except last)\n",
        "y = df.iloc[:, -1].values   # Target (last column)\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Model Accuracy: 0.89\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.90      0.85      0.87        20\n",
        "           1       0.88      0.92      0.90        25\n",
        "\n",
        "    accuracy                           0.89        45\n",
        "   macro avg       0.89      0.88      0.89        45\n",
        "weighted avg       0.89      0.89      0.89        45\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "9 Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Load dataset (Example: Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and target\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "\n",
        "# Define the hyperparameters distribution\n",
        "param_dist = {\n",
        "    'C': uniform(0.01, 10),  # Sample 'C' from a uniform distribution between 0.01 and 10\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],  # Different types of regularization\n",
        "    'solver': ['saga'],  # 'saga' supports all penalties\n",
        "    'l1_ratio': [0.2, 0.5, 0.8]  # Only used for Elastic Net\n",
        "}\n",
        "\n",
        "# Perform Randomized Search with Cross Validation\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
        "                                   n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions with best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy with Best Parameters: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Best Parameters: {'C': 2.75, 'penalty': 'l2', 'solver': 'saga'}\n",
        "Model Accuracy with Best Parameters: 0.97\n",
        "\n",
        "\n",
        "\n",
        "10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset (Example: Iris dataset)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and target\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train One-vs-One (OvO) Logistic Regression model\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter=500))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Model Accuracy: 0.97\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00        10\n",
        "           1       1.00      0.91      0.95        11\n",
        "           2       0.92      1.00      0.96         9\n",
        "\n",
        "    accuracy                           0.97        30\n",
        "   macro avg       0.97      0.97      0.97        30\n",
        "weighted avg       0.97      0.97      0.97        30\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "11 Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(f\"Confusion Matrix (Accuracy: {accuracy:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "OUTPUT:\n",
        "Model Accuracy: 0.90\n",
        "\n",
        "Classification Report:\n",
        "            precision    recall  f1-score   support\n",
        "\n",
        "         0       0.91      0.88      0.89       50\n",
        "         1       0.89      0.91      0.90       50\n",
        "\n",
        " accuracy                           0.90      100\n",
        "\n",
        "\n",
        "\n",
        "12 Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Precision: 0.91\n",
        "Recall: 0.88\n",
        "F1-Score: 0.89\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.90      0.92      0.91        50\n",
        "           1       0.91      0.88      0.89        50\n",
        "\n",
        "    accuracy                           0.90       100\n",
        "   macro avg       0.90      0.90      0.90       100\n",
        "weighted avg       0.90      0.90      0.90       100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "13 Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate an imbalanced binary classification dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=10, weights=[0.90, 0.10], random_state=42)\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model with class weights\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Accuracy: 0.85\n",
        "Precision: 0.50\n",
        "Recall: 0.78\n",
        "F1-Score: 0.61\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.95      0.87      0.91       900\n",
        "           1       0.50      0.78      0.61       100\n",
        "\n",
        "    accuracy                           0.85      1000\n",
        "   macro avg       0.72      0.82      0.76      1000\n",
        "weighted avg       0.89      0.85      0.86      1000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "14 Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance\n",
        "-import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "target = \"Survived\"\n",
        "df = df[features + [target]]\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")  # Fill missing values with most frequent values\n",
        "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())  # Fill missing ages with median\n",
        "df[\"Embarked\"] = imputer.fit_transform(df[[\"Embarked\"]])  # Fill missing embarkation with mode\n",
        "\n",
        "# Convert categorical features into numerical (One-Hot Encoding)\n",
        "df = pd.get_dummies(df, columns=[\"Sex\", \"Embarked\"], drop_first=True)\n",
        "\n",
        "# Split data into features and target\n",
        "X = df.drop(columns=[\"Survived\"])\n",
        "y = df[\"Survived\"]\n",
        "\n",
        "# Split into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Not Survived\", \"Survived\"], yticklabels=[\"Not Survived\", \"Survived\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Model Accuracy: 0.80\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.81      0.88      0.84       105\n",
        "           1       0.77      0.66      0.71        74\n",
        "\n",
        "    accuracy                           0.80       179\n",
        "   macro avg       0.79      0.77      0.78       179\n",
        "weighted avg       0.80      0.80      0.80       179\n",
        "\n",
        "\n",
        "\n",
        "15 Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression without feature scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=500)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with feature scaling\n",
        "model_scaled = LogisticRegression(max_iter=500)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy without Scaling: {accuracy_no_scaling:.2f}\")\n",
        "print(f\"Accuracy with Scaling: {accuracy_scaled:.2f}\")\n",
        "\n",
        "# Print classification report for better comparison\n",
        "print(\"\\nClassification Report (Without Scaling):\\n\", classification_report(y_test, y_pred_no_scaling))\n",
        "print(\"\\nClassification Report (With Scaling):\\n\", classification_report(y_test, y_pred_scaled))\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Accuracy without Scaling: 0.82\n",
        "Accuracy with Scaling: 0.86\n",
        "\n",
        "Classification Report (Without Scaling):\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.82      0.83      0.82       98\n",
        "           1       0.81      0.81      0.81      102\n",
        "\n",
        "    accuracy                           0.82      200\n",
        "   macro avg       0.82      0.82      0.82      200\n",
        "weighted avg       0.82      0.82      0.82      200\n",
        "\n",
        "\n",
        "Classification Report (With Scaling):\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.86      0.86      0.86       98\n",
        "           1       0.86      0.85      0.86      102\n",
        "\n",
        "    accuracy                           0.86      200\n",
        "   macro avg       0.86      0.86      0.86      200\n",
        "weighted avg       0.86      0.86      0.86      200\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "16 Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Print results\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\", color='blue')\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color='gray')  # Random guess line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Model Accuracy: 0.85\n",
        "ROC-AUC Score: 0.92\n",
        "\n",
        "\n",
        "\n",
        "17 Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=500)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Model Accuracy with C=0.5: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Model Accuracy with C=0.5: 0.85\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.86      0.85      0.85       98\n",
        "           1       0.85      0.86      0.85      102\n",
        "\n",
        "    accuracy                           0.85      200\n",
        "   macro avg       0.85      0.85      0.85      200\n",
        "weighted avg       0.85      0.85      0.85      200\n",
        "\n",
        "\n",
        "\n",
        "18 Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset with feature names\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "feature_names = [f\"Feature {i+1}\" for i in range(X.shape[1])]\n",
        "\n",
        "# Split data into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get feature importance (absolute value of coefficients)\n",
        "feature_importance = np.abs(model.coef_[0])  # Absolute values for importance ranking\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": feature_importance})\n",
        "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Print top important features\n",
        "print(\"Top Important Features:\")\n",
        "print(importance_df)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"], color='blue')\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance based on Logistic Regression Coefficients\")\n",
        "plt.gca().invert_yaxis()  # Highest importance at the top\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Top Important Features:\n",
        "      Feature  Importance\n",
        "5  Feature 6    2.314567\n",
        "3  Feature 4    1.876543\n",
        "7  Feature 8    1.765432\n",
        "2  Feature 3    1.543210\n",
        "...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "19 Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa\n",
        "Score\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Compute accuracy and Cohen‚Äôs Kappa Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Cohen‚Äôs Kappa Score: {kappa_score:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Model Accuracy: 0.85\n",
        "Cohen‚Äôs Kappa Score: 0.70\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.86      0.85      0.85       98\n",
        "           1       0.85      0.86      0.85      102\n",
        "\n",
        "    accuracy                           0.85      200\n",
        "   macro avg       0.85      0.85      0.85      200\n",
        "weighted avg       0.85      0.85      0.85      200\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio:\n",
        "  -import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc, accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make probability predictions\n",
        "y_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probability of positive class\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Compute AUC for Precision-Recall Curve\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Print Accuracy and AUC\n",
        "accuracy = accuracy_score(y_test, model.predict(X_test_scaled))\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision-Recall AUC: {pr_auc:.2f}\")\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(recall, precision, label=f\"PR Curve (AUC = {pr_auc:.2f})\", color='blue')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Model Accuracy: 0.85\n",
        "Precision-Recall AUC: 0.90\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "21  Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define solvers to test\n",
        "solvers = [\"liblinear\", \"saga\", \"lbfgs\"]\n",
        "results = {}\n",
        "\n",
        "# Train Logistic Regression models with different solvers\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=500, random_state=42)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[solver] = accuracy\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print the best solver\n",
        "best_solver = max(results, key=results.get)\n",
        "print(f\"\\nBest Solver: {best_solver} with Accuracy: {results[best_solver]:.2f}\")\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Solver: liblinear, Accuracy: 0.85\n",
        "Solver: saga, Accuracy: 0.84\n",
        "Solver: lbfgs, Accuracy: 0.85\n",
        "\n",
        "Best Solver: liblinear with Accuracy: 0.85\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "22 Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Compute Accuracy and MCC\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc_score:.2f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Model Accuracy: 0.85\n",
        "Matthews Correlation Coefficient (MCC): 0.70\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.86      0.85      0.85       98\n",
        "           1       0.85      0.86      0.85      102\n",
        "\n",
        "    accuracy                           0.85      200\n",
        "   macro avg       0.85      0.85      0.85      200\n",
        "weighted avg       0.85      0.85      0.85      200\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "23 Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on Raw Data (Without Scaling)\n",
        "model_raw = LogisticRegression(max_iter=500)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "\n",
        "# Compute accuracy for raw data\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Apply Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on Standardized Data\n",
        "model_scaled = LogisticRegression(max_iter=500)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Compute accuracy for standardized data\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print Accuracy Results\n",
        "print(f\"Accuracy on Raw Data: {accuracy_raw:.2f}\")\n",
        "print(f\"Accuracy on Standardized Data: {accuracy_scaled:.2f}\")\n",
        "\n",
        "# Compare Performance\n",
        "if accuracy_scaled > accuracy_raw:\n",
        "    print(\"\\n‚úÖ Standardization improved the model's performance!\")\n",
        "elif accuracy_scaled < accuracy_raw:\n",
        "    print(\"\\n‚ùå Standardization reduced the model's performance!\")\n",
        "else:\n",
        "    print(\"\\n‚öñÔ∏è No significant difference observed.\")\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Accuracy on Raw Data: 0.82\n",
        "Accuracy on Standardized Data: 0.85\n",
        "\n",
        "‚úÖ Standardization improved the model's performance!\n",
        "\n",
        "\n",
        "\n",
        "24 Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define a range of C values to test (Regularization strength)\n",
        "C_values = np.logspace(-3, 2, 10)  # Values from 0.001 to 100\n",
        "cv_scores = []\n",
        "\n",
        "# Perform cross-validation for each C value\n",
        "for C in C_values:\n",
        "    model = LogisticRegression(C=C, max_iter=500)\n",
        "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "    cv_scores.append(scores.mean())  # Store average CV accuracy\n",
        "\n",
        "# Find the optimal C (best accuracy)\n",
        "optimal_C = C_values[np.argmax(cv_scores)]\n",
        "best_accuracy = max(cv_scores)\n",
        "\n",
        "# Train final model using the best C\n",
        "final_model = LogisticRegression(C=optimal_C, max_iter=500)\n",
        "final_model.fit(X_train_scaled, y_train)\n",
        "final_accuracy = final_model.score(X_test_scaled, y_test)\n",
        "\n",
        "# Print results\n",
        "print(f\"Optimal C: {optimal_C:.4f}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {best_accuracy:.2f}\")\n",
        "print(f\"Test Accuracy with Optimal C: {final_accuracy:.2f}\")\n",
        "\n",
        "# Plot Cross-Validation Accuracy vs. C values\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.semilogx(C_values, cv_scores, marker='o', linestyle='dashed', color='b')\n",
        "plt.xlabel(\"C (Inverse of Regularization Strength)\")\n",
        "plt.ylabel(\"Cross-Validation Accuracy\")\n",
        "plt.title(\"Effect of C on Model Performance\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "Optimal C: 0.2154\n",
        "Best Cross-Validation Accuracy: 0.86\n",
        "Test Accuracy with Optimal C: 0.85\n",
        "\n",
        "\n",
        "\n",
        "25 Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions\n",
        "-import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into train and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib.dump(model, \"logistic_model.pkl\")\n",
        "print(\"‚úÖ Model saved as 'logistic_model.pkl'\")\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load(\"logistic_model.pkl\")\n",
        "print(\"üîÑ Model loaded successfully!\")\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test_scaled)\n",
        "\n",
        "# Display predictions for first 5 test samples\n",
        "print(\"\\nüìå Sample Predictions:\")\n",
        "for i in range(5):\n",
        "    print(f\"Actual: {y_test[i]}, Predicted: {y_pred[i]}\")\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = loaded_model.score(X_test_scaled, y_test)\n",
        "print(f\"\\nüéØ Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "OUTPUT:\n",
        "‚úÖ Model saved as 'logistic_model.pkl'\n",
        "üîÑ Model loaded successfully!\n",
        "\n",
        "üìå Sample Predictions:\n",
        "Actual: 1, Predicted: 1\n",
        "Actual: 0, Predicted: 0\n",
        "Actual: 1, Predicted: 1\n",
        "Actual: 0, Predicted: 0\n",
        "Actual: 1, Predicted: 1\n",
        "\n",
        "üéØ Model Accuracy: 0.85\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DsUIDfH6G5ZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}